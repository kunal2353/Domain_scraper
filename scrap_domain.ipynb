{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXHqp7c6HXKm",
        "outputId": "48ee7c91-cf95-47e5-bfed-8b4ee3bd6216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas\n",
        "#beautifulsoup4 helps Python read and understand website HTML code and extract required data from it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "#I want Python to open websites for me."
      ],
      "metadata": {
        "id": "51v4IJ7-Iu1l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "#i want python code to read and understand html code"
      ],
      "metadata": {
        "id": "FH_oBPw6JRuz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#i want to work with dataframe and csv files"
      ],
      "metadata": {
        "id": "doF9UOXEJelJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#i want to interact with files and folders as this code is not used in this, but it is used for future"
      ],
      "metadata": {
        "id": "8ynQSglqJMTx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.whoisds.com/newly-registered-domains/\"\n",
        "#stores the website link in a variable"
      ],
      "metadata": {
        "id": "VceJWanoKDdD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_domains():#function name\n",
        "\n",
        "  response = requests.get(URL)#sends a request and gets html content\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')#Make website code understandable\n",
        "      # Like converting PDF â†’ Word for easy reading.\n",
        "  domain_list = []#creates an empty list\n",
        "  table = soup.find(\"div\", class_=\"domain-name\")#go inside the html and find what is written inside the <div>\n",
        "\n",
        "  if not table:\n",
        "    print(\"no data is found\")\n",
        "    return []\n",
        "  links = table.find_all(\"a\")#find all links and in html it is anchor tag </a>\n",
        "  for l in links:\n",
        "    domain = l.text.strip()#extract domain texts and removes spaces\n",
        "\n",
        "    domain_list.append({\n",
        "        \"domain_name\":\"domain\",\n",
        "        \"date\":pd.Timestamp.Today().date(),\n",
        "        \"country\":\"unknown\",\n",
        "        \"website\": f\"http://{domain}\",\n",
        "        \"email\":\"not available\"\n",
        "\n",
        "    })\n",
        "  return domain_list\n",
        "\n"
      ],
      "metadata": {
        "id": "1dqq9f5eKNYR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(data):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.drop_duplicates(subset=\"domain_name\", inplace=True)\n",
        "    df.to_csv(\"domains.csv\", index=False)\n",
        "    print(\"CSV created!\")\n",
        "\n",
        "\n",
        "domains = scrap_domains()\n",
        "save_to_csv(domains)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCaAiOstMEpC",
        "outputId": "fd612e2e-5afc-438a-b886-553503d4254c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no data is found\n",
            "CSV created!\n"
          ]
        }
      ]
    }
  ]
}